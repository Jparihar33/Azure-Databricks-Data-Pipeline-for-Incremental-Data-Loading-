# Azure-Databricks-Data-Pipeline-for-Incremental-Data-Loading

Data Engineering Project with PySpark, Delta Lake & DLT
Work on a complete Data Engineering project using powerful tools like Azure Databricks, Delta Live Tables (DLT), Spark Structured Streaming, and PySpark.
You'll explore real-world enterprise use cases such as:

âœ… Dimensional Data Modeling

âœ… Handling Slowly Changing Dimensions (SCD Type 1 & 2)

âœ… Using Databricks to build scalable, governed pipelines

This project demonstrates a robust, end-to-end pipeline that integrates batch and streaming data, supports historical tracking with surrogate keys, and aligns with lakehouse architecture principles.

ðŸ“¦ Features
âœ… Streaming ingestion using Spark Structured Streaming

âœ… SCD Type 1 and SCD Type 2 logic implemented manually and via DLT

âœ… Surrogate key handling and foreign key reassignment

âœ… Use of Delta Lake for ACID-compliant tables

âœ… Optional implementation with Delta Live Tables (DLT)

âœ… Designed for production-grade lakehouse pipelines

âœ… Covers key data engineering concepts like lineage, versioning, and schema enforcement

